{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71c99ac-4d6d-4621-99ba-e685a650f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielrubibreton/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library ===\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict, List, Optional, Union, TypedDict, Annotated\n",
    "\n",
    "# === Data Handling ===\n",
    "import pandas as pd\n",
    "\n",
    "# === NLP Models ===\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# === LangGraph Core ===\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# === LangChain & LLM ===\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "import ollama  # used directly in custom LLM calls\n",
    "\n",
    "# === Validation ===\n",
    "from pydantic import BaseModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f4b33-d3d1-4feb-b459-eff5d132968d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d9333-842e-40c5-8391-60a5362ec869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcdbf3c-bff6-4918-b492-7e1ef13318ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcabb40-8737-4d3f-bfe5-a79c4b16cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/danielrubibreton/Desktop/PythonStuff/hface/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac749648-d74a-433a-bae0-396e0e3c5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec76f18-7163-4502-9599-cccf2bc25c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf35d71-3dba-475f-8f3b-e648a8e0b26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "HumanMessage(content=\"Hello\").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53382777-67f6-43be-a9a5-fe243fb57699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09f4cb-efee-4df3-a299-271f55021d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ece89dc-284b-4a0e-9304-1331a3d655ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cfa2025.json\", \"r\") as file:\n",
    "    book_json = eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a17dc5-ef24-40bb-ac6c-71642fdd8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_sections = [f\"{book} -> {chapter}\" for book, chapters in book_json.items() for chapter in chapters]\n",
    "\n",
    "flat_book = [f\"{book} -> {chapter} -> {book_json[book][chapter] }\" for book, chapters in book_json.items() for chapter in chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06db66b-8545-434d-ad3e-0461f8502431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df4a425-853e-4367-9f8e-cb9a58245873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_sections(\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    score_threshold1: float = 0.5,\n",
    "    score_threshold2: float = 0.3,\n",
    "    model_name: str = 'all-MiniLM-L6-v2',\n",
    "    return_content: bool = False\n",
    ") -> Union[List[str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    If return_content=False: return top_k section titles (e.g. [\"Genesis -> 1\", ...]).\n",
    "    If return_content=True: return a dict mapping each section title to its full text.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name, device='mps')\n",
    "    query_emb = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # 1) Title‐level matching\n",
    "    sec_embs = model.encode(flat_sections, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(query_emb, sec_embs)[0]\n",
    "    hits = sorted(\n",
    "        [(i, s.item()) for i, s in enumerate(scores) if s.item() >= score_threshold1],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )[:top_k]\n",
    "\n",
    "    if not hits:\n",
    "        # 2) Fallback: book‐level matching\n",
    "        book_embs = model.encode(flat_book, convert_to_tensor=True)\n",
    "        scores = util.cos_sim(query_emb, book_embs)[0]\n",
    "        hits = sorted(\n",
    "            [(i, s.item()) for i, s in enumerate(scores) if s.item() >= score_threshold2],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )[: top_k + 2]\n",
    "        # normalize to just \"Book -> Chapter\"\n",
    "        section_keys = []\n",
    "        for idx, _ in hits:\n",
    "            book, chap, _ = flat_book[idx].split(\" -> \", 2)\n",
    "            section_keys.append(f\"{book} -> {chap}\")\n",
    "    else:\n",
    "        section_keys = [flat_sections[i] for i, _ in hits]\n",
    "\n",
    "    if not return_content:\n",
    "        return section_keys\n",
    "\n",
    "    # if return_content=True, build the full dict\n",
    "    return {\n",
    "        sec: book_json[sec.split(\" -> \", 1)[0]][sec.split(\" -> \", 1)[1]]\n",
    "        for sec in section_keys\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072829a4-fd9c-4442-b1d6-c35f126d5f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 534 ms, total: 1.69 s\n",
      "Wall time: 3.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Fixed Income -> 2.1.Maturity Structure of Interest Rates',\n",
       " 'Quantitative Methods -> 2.Interest Rates and Time Value of Money',\n",
       " 'Fixed Income -> 2.2.Yield-to-Maturity',\n",
       " 'Quantitative Methods -> 2.1.Determinants of Interest Rates']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "find_relevant_sections(\"What is interest rate (or yield)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9af5a9b-4288-4535-827e-4b1e70d85394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"deepseek-r1:1.5b\",\n",
    "        temperature: float = 0,\n",
    "        max_tokens: Optional[int] = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            options={\n",
    "                \"temperature\": self.temperature,\n",
    "                \"num_thread\": 10,\n",
    "                \"low_vram\": False,\n",
    "            } \n",
    "        )\n",
    "        return response[\"message\"][\"content\"].split(\"</think>\")[-1].strip(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82bb1a6b-af71-4957-84be-996f073fd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LocalLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69534908-4c36-4fff-81ef-72e999326030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. GraphState definition\n",
    "# -------------------------------------------------------------------------\n",
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    retrieved_sections: Optional[Any]       # Will hold either List[str] or str\n",
    "    response: Optional[str]\n",
    "    messages: Annotated[List[Dict[str, str]], add_messages]\n",
    "    goto: Optional[str]          # Used by the confirm node to drive conditional edges\n",
    "    context: str\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Retrieval node (unchanged)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def user_query_node(state: GraphState) -> GraphState:\n",
    "    query = state[\"query\"]\n",
    "    # find_relevant_sections(query) returns List[str]\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=query)]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def retrieval_node(state: GraphState) -> GraphState:\n",
    "    query = state[\"query\"]\n",
    "    # find_relevant_sections(query) returns List[str]\n",
    "    section_list = find_relevant_sections(query)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [SystemMessage(content=section_list)],\n",
    "        \"retrieved_sections\": section_list\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Confirm node using interrupt()\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def confirm_node(state: GraphState) -> Dict[str, Any]:\n",
    "    sections = state[\"retrieved_sections\"]\n",
    "\n",
    "    # 1) Exactly one → skip confirmation\n",
    "    if len(sections) == 1:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                SystemMessage(content=f\"Only one section found: {sections[0]}. Skipping confirmation.\")\n",
    "            ],\n",
    "            \"retrieved_sections\": sections,\n",
    "            \"goto\": \"full_retrieval\"\n",
    "        }\n",
    "\n",
    "    # 2) None → ask user to rephrase query\n",
    "    if not sections:\n",
    "        new_q = interrupt({\n",
    "            \"prompt\": \"I couldn't find any sections matching your question. Please rephrase or clarify your query.\"\n",
    "        })\n",
    "        return {\n",
    "            \"messages\": [SystemMessage(content=f\"User provided new query: {new_q}\")],\n",
    "            \"query\": new_q,\n",
    "            \"goto\": \"retrieve\"\n",
    "        }\n",
    "\n",
    "    # 3) Multiple → present numbered options\n",
    "    opts = \"\\n\".join(f\"{i+1}. {sec}\" for i, sec in enumerate(sections))\n",
    "    choice = interrupt({\n",
    "        \"prompt\": (\n",
    "            \"I found multiple relevant sections. Please select one by number:\\n\\n\"\n",
    "            f\"{opts}\\n\\nReply with 1, 2, 3, etc.\"\n",
    "        )\n",
    "    })\n",
    "\n",
    "    # validate\n",
    "    try:\n",
    "        idx = int(choice.strip()) - 1\n",
    "        if idx < 0 or idx >= len(sections):\n",
    "            raise ValueError()\n",
    "    except ValueError:\n",
    "        return {\n",
    "            \"messages\": [SystemMessage(content=f\"Invalid choice '{choice}'. Let me try again.\")],\n",
    "            \"goto\": \"confirm\"\n",
    "        }\n",
    "\n",
    "    # valid selection\n",
    "    picked = sections[idx]\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=f\"User selected option {idx+1}: {picked}\")\n",
    "        ],\n",
    "        \"retrieved_sections\": [picked],\n",
    "        \"goto\": \"full_retrieval\"\n",
    "    }\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Full retrieval node (unchanged)\n",
    "# -------------------------------------------------------------------------\n",
    "def full_retrieval_node(state: GraphState) -> GraphState:\n",
    "    section_list = state[\"retrieved_sections\"]  # This is still List[str]\n",
    "    all_text = []\n",
    "    for sec in section_list:\n",
    "        book, chap = sec.split(\" -> \", 1)\n",
    "        text = book_json[book][chap]\n",
    "        full_ctx.append(f\"Book & Chapter: {sec}\\n{text}\")\n",
    "        \n",
    "    concatenated = \"\\n\".join(all_text)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"context\": concatenated,\n",
    "        \"messages\": [SystemMessage(content=\"Fetched full text for all candidate sections.\")]\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. Response node (unchanged)\n",
    "# -------------------------------------------------------------------------\n",
    "def response_node(state: GraphState) -> GraphState:\n",
    "    context_text = state[\"context\"]  # Now a big string\n",
    "    query = state[\"query\"]\n",
    "    prompt = (\n",
    "        \"Below are the relevant CFA curriculum sections:\\n\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Please answer in a concise, yet complete manner, citing the relevant sections as needed.\"\n",
    "    )\n",
    "    llm_answer = llm.invoke(prompt)\n",
    "\n",
    "    return {\n",
    "        \"response\": llm_answer,\n",
    "        \"messages\": [AIMessage(content=llm_answer)]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ec548-d564-4dda-a125-70fab1dd8a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1f73e-b2c8-4f93-abd6-989e068085d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85eec45-ae70-45e6-8cd5-481389453f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354bce94-ff8d-476c-93d2-944945abc218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c32e4-c79a-4ceb-a766-fa3166042904",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = (\n",
    "    StateGraph(GraphState)\n",
    "    .add_node(\"retrieve\",       retrieval_node)\n",
    "    .add_node(\"confirm\",        confirm_node)\n",
    "    .add_node(\"full_retrieval\", full_retrieval_node)\n",
    "    .add_node(\"respond\",        response_node)\n",
    "    \n",
    "    .add_edge(START,            \"retrieve\")\n",
    "    .add_edge(\"retrieve\",       \"confirm\")\n",
    "    .add_conditional_edges(\n",
    "        \"confirm\",\n",
    "        path=lambda out: out.get(\"goto\"),\n",
    "        path_map={\"full_retrieval\":\"full_retrieval\",\"retrieve\":\"retrieve\"},\n",
    "    )\n",
    "    .add_edge(\"full_retrieval\", \"respond\")\n",
    "    .add_edge(\"respond\",         END)\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "graph.get_graph().print_ascii()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c5ec6-88c1-46c8-abb9-70eeeb385d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e108c1eb-d4e9-4f94-9a85-de8f098a6532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a496702-e3be-46f3-9194-a86d0653a043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ad30a-15c1-4828-8762-1b345db6a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume `graph` is already defined and compiled as in your notebook\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"You: What is the difference between Venture capital and Private Equity?\")\n",
    "    if user_query.strip().lower() == \"exit\":\n",
    "        print(\"Exiting chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # build the LangGraph input\n",
    "    state = {\"messages\": [{\"role\": \"user\", \"content\": user_query}]}\n",
    "    \n",
    "    # stream the response tokens as they arrive\n",
    "    for msg_chunk, meta in graph.stream(\n",
    "        state,\n",
    "        stream_mode=\"messages\"\n",
    "    ):\n",
    "        print(meta.get(\"langgraph_node\"))\n",
    "        # only print from your 'respond' node\n",
    "        if meta.get(\"langgraph_node\") == \"respond\" and msg_chunk.content:\n",
    "            print(msg_chunk.content, end=\"\", flush=True)\n",
    "    print()  # newline after the full response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add293c2-758a-496a-a128-ea2bc66a114f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d34c8f-e3cc-48c7-8fb3-8ef70c6ac82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07152c31-466c-4ee5-a360-56e4e81f7172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc53ba7-852f-4bc9-8f7b-5662b13bedc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf151b-1601-42ab-9104-e86e0ae87e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22be07d-501d-4b3b-bd78-b8c364873449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2469606-3878-4494-ae15-2e313180c17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1f996-9445-45da-bcca-6c2bc0836f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea00a35-2485-4546-acec-f605f865be64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

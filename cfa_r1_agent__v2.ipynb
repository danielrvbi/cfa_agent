{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71c99ac-4d6d-4621-99ba-e685a650f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, sys\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from typing import Union, List, Dict\n",
    "\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel\n",
    "import ollama\n",
    "\n",
    "#from langchain.chat_models import ChatOllama\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d64c869-ac5b-40b5-a55f-3202c64b1c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/danielrubibreton/Documents/CFA'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07d9333-842e-40c5-8391-60a5362ec869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efcabb40-8737-4d3f-bfe5-a79c4b16cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/danielrubibreton/Desktop/PythonStuff/hface/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac749648-d74a-433a-bae0-396e0e3c5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec76f18-7163-4502-9599-cccf2bc25c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf35d71-3dba-475f-8f3b-e648a8e0b26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53382777-67f6-43be-a9a5-fe243fb57699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09f4cb-efee-4df3-a299-271f55021d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce95e8-a9bf-4e24-ba4a-b219ce74a4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c657d61a-5528-4e5d-a0e4-92e7ba5c4ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2eacf-28f7-4c96-8f7b-44608ed0fb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c424c6-cf95-4ff7-b89a-6d80873ccffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ee123-40e2-44ed-8eab-bd7cd4a8d85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf1691-f53e-41c4-89b6-3abaef4bea3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63f396-00f3-479e-b671-3c8204a61ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9527f4-6748-45ad-824e-25335903ee98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ece89dc-284b-4a0e-9304-1331a3d655ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cfa2025.json\", \"r\") as file:\n",
    "    book_json = eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a17dc5-ef24-40bb-ac6c-71642fdd8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_sections = [f\"{book} -> {chapter}\" for book, chapters in book_json.items() for chapter in chapters]\n",
    "\n",
    "flat_book = [f\"{book} -> {chapter} -> {book_json[book][chapter] }\" for book, chapters in book_json.items() for chapter in chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06db66b-8545-434d-ad3e-0461f8502431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "be0f6f5d-9e8a-4605-8981-6c8eb346ec59",
   "metadata": {},
   "source": [
    "# Enhanced function with fallback to content-level matching\n",
    "def find_relevant_sections_v1(\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        score_threshold1: float = 0.5,\n",
    "        score_threshold2: float = 0.3, \n",
    "        model_name: str = 'all-MiniLM-L6-v2'\n",
    "    ) -> Dict[str, str]:\n",
    "\n",
    "    model = SentenceTransformer(model_name, device='mps')\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # First: match against subtopic titles\n",
    "    section_embeddings = model.encode(flat_sections, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, section_embeddings)[0]\n",
    "    above_threshold = [(i, score.item()) for i, score in enumerate(cosine_scores) if score.item() >= score_threshold1]\n",
    "    sorted_above_threshold = sorted(above_threshold, key=lambda x: x[1], reverse=True)\n",
    "    top_results = sorted_above_threshold[:top_k]\n",
    "\n",
    "    if top_results:\n",
    "        relevant_content = {}\n",
    "        for idx, score in top_results:\n",
    "            section_name = flat_sections[idx]\n",
    "            book, chapter = section_name.split(\" -> \", 1)\n",
    "            relevant_content[section_name] = book_json[book][chapter]\n",
    "        return relevant_content\n",
    "\n",
    "    book_embeddings = model.encode(flat_book, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, book_embeddings)[0]\n",
    "    above_threshold = [(i, score.item()) for i, score in enumerate(cosine_scores) if score.item() >= score_threshold2]\n",
    "    sorted_above_threshold = sorted(above_threshold, key=lambda x: x[1], reverse=True)\n",
    "    top_results = sorted_above_threshold[:(top_k+2)]\n",
    "\n",
    "    relevant_content = {}\n",
    "    for idx, score in top_results:\n",
    "        entry = flat_book[idx]  # format: book -> chapter -> content\n",
    "        book, chapter, _ = entry.split(\" -> \", 2)\n",
    "        relevant_content[f\"{book} -> {chapter}\"] = book_json[book][chapter]\n",
    "\n",
    "    return relevant_content"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de40d66b-f394-4884-86c7-613a39fc8325",
   "metadata": {},
   "source": [
    "%%time\n",
    "res = find_relevant_sections(\"What is interest rate (or yield)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df4a425-853e-4367-9f8e-cb9a58245873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_sections(\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    score_threshold1: float = 0.5,\n",
    "    score_threshold2: float = 0.3,\n",
    "    model_name: str = 'all-MiniLM-L6-v2',\n",
    "    return_content: bool = False\n",
    ") -> Union[List[str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    If return_content=False: return top_k section titles (e.g. [\"Genesis -> 1\", ...]).\n",
    "    If return_content=True: return a dict mapping each section title to its full text.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name, device='mps')\n",
    "    query_emb = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # 1) Title‐level matching\n",
    "    sec_embs = model.encode(flat_sections, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(query_emb, sec_embs)[0]\n",
    "    hits = sorted(\n",
    "        [(i, s.item()) for i, s in enumerate(scores) if s.item() >= score_threshold1],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )[:top_k]\n",
    "\n",
    "    if not hits:\n",
    "        # 2) Fallback: book‐level matching\n",
    "        book_embs = model.encode(flat_book, convert_to_tensor=True)\n",
    "        scores = util.cos_sim(query_emb, book_embs)[0]\n",
    "        hits = sorted(\n",
    "            [(i, s.item()) for i, s in enumerate(scores) if s.item() >= score_threshold2],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )[: top_k + 2]\n",
    "        # normalize to just \"Book -> Chapter\"\n",
    "        section_keys = []\n",
    "        for idx, _ in hits:\n",
    "            book, chap, _ = flat_book[idx].split(\" -> \", 2)\n",
    "            section_keys.append(f\"{book} -> {chap}\")\n",
    "    else:\n",
    "        section_keys = [flat_sections[i] for i, _ in hits]\n",
    "\n",
    "    if not return_content:\n",
    "        return section_keys\n",
    "\n",
    "    # if return_content=True, build the full dict\n",
    "    return {\n",
    "        sec: book_json[sec.split(\" -> \", 1)[0]][sec.split(\" -> \", 1)[1]]\n",
    "        for sec in section_keys\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "072829a4-fd9c-4442-b1d6-c35f126d5f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 592 ms, total: 1.82 s\n",
      "Wall time: 6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = find_relevant_sections(\"What is interest rate (or yield)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3b7b06-e410-47e7-a803-45e473d98b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fixed Income -> 2.1.Maturity Structure of Interest Rates',\n",
       " 'Quantitative Methods -> 2.Interest Rates and Time Value of Money',\n",
       " 'Fixed Income -> 2.2.Yield-to-Maturity',\n",
       " 'Quantitative Methods -> 2.1.Determinants of Interest Rates']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9af5a9b-4288-4535-827e-4b1e70d85394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"deepseek-r1:1.5b\",\n",
    "        temperature: float = 0,\n",
    "        max_tokens: Optional[int] = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            options={\n",
    "                \"temperature\": self.temperature,\n",
    "                \"num_thread\": 10,\n",
    "                \"low_vram\": False,\n",
    "            } \n",
    "        )\n",
    "        return response[\"message\"][\"content\"].split(\"</think>\")[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69534908-4c36-4fff-81ef-72e999326030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    query: str\n",
    "    context: Optional[str]\n",
    "    response: Optional[str]\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860b0d79-53fb-4e05-890e-3e570925e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_node(state: GraphState) -> dict:\n",
    "    try:\n",
    "        query = state[\"messages\"][-1].content\n",
    "    except:\n",
    "        query = state[\"query\"]\n",
    "\n",
    "    \n",
    "    # only pull titles\n",
    "    section_keys = find_relevant_sections(query)\n",
    "    # store selected section keys for later full load\n",
    "\n",
    "    titles = \"\\n\".join(f\"{i+1}. {sec}\" for i, sec in enumerate(section_keys))\n",
    "    # ask user to confirm\n",
    "    prompt = (\n",
    "        \"I found these sections for your query:\\n\\n\"\n",
    "        f\"{titles}\\n\\n\"\n",
    "        \"Are these the sections you want to use? (yes/no)\"\n",
    "    )\n",
    "    # send as assistant message\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"section_keys\": section_keys,   # stash for full load\n",
    "    }\n",
    "\n",
    "def full_retrieval_node(state: GraphState) -> dict:\n",
    "    # load full text only after confirmation\n",
    "    keys = state[\"section_keys\"]\n",
    "    full_ctx = []\n",
    "    for sec in keys:\n",
    "        book, chap = sec.split(\" -> \", 1)\n",
    "        text = book_json[book][chap]\n",
    "        full_ctx.append(f\"Book & Chapter: {sec}\\n{text}\")\n",
    "    context = \"\\n\\n\".join(full_ctx)\n",
    "    return {\"context\": context}\n",
    "\n",
    "# def confirm_node(state: GraphState) -> dict:\n",
    "#     reply = state[\"messages\"][-1].content.strip().lower()\n",
    "#     if reply in (\"yes\", \"y\", \"ok\", \"sure\"):\n",
    "#         # proceed to full load\n",
    "#         return {\"goto\": \"full_retrieval\"}\n",
    "#     else:\n",
    "#         # either cancel or re-run retrieval\n",
    "#         return {\"goto\": \"retrieve\", \"prompt\": \"Okay, let me try again. What would you like to search for?\"}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b59d786b-8e59-4296-a65b-b468a6150ef6",
   "metadata": {},
   "source": [
    "ollama.chat(model='deepseek-r1:1.5b', messages=[\n",
    "        {\"role\": \"user\", \"content\": \"who am I?\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8430a1c8-7a29-42cd-85f5-64286ce9d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_node(state: GraphState) -> dict:\n",
    "    query = state.query\n",
    "    context = state.context\n",
    "    \n",
    "    prompt = (\n",
    "        \"Answer the following question using only the provided context. \"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question:\\n{query}\"\n",
    "        \"Cite your sources like based on the context, if there is no specific info, answer there is no info\"\n",
    "    )\n",
    "\n",
    "    llm = LocalLLM(model=\"deepseek-r1:1.5b\")\n",
    "    response_text = llm.invoke(prompt)\n",
    "\n",
    "    # Append the assistant's response to the messages\n",
    "    new_message = {\"role\": \"assistant\", \"content\": response_text}\n",
    "    updated_messages = state[\"messages\"] + [new_message]\n",
    "\n",
    "    return {\"response\": response_text, \"messages\": updated_messages}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007c32e4-c79a-4ceb-a766-fa3166042904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  +-----------+    \n",
      "  | __start__ |    \n",
      "  +-----------+    \n",
      "         *         \n",
      "         *         \n",
      "         *         \n",
      "   +----------+    \n",
      "   | retrieve |    \n",
      "   +----------+    \n",
      "         *         \n",
      "         *         \n",
      "         *         \n",
      "+----------------+ \n",
      "| full_retrieval | \n",
      "+----------------+ \n",
      "         *         \n",
      "         *         \n",
      "         *         \n",
      "    +---------+    \n",
      "    | respond |    \n",
      "    +---------+    \n",
      "         *         \n",
      "         *         \n",
      "         *         \n",
      "    +---------+    \n",
      "    | __end__ |    \n",
      "    +---------+    \n"
     ]
    }
   ],
   "source": [
    "graph = (\n",
    "    StateGraph(GraphState)\n",
    "    .add_node(\"retrieve\",       retrieval_node)\n",
    "    .add_node(\"confirm\",        confirm_node)\n",
    "    .add_node(\"full_retrieval\", full_retrieval_node)\n",
    "    .add_node(\"respond\",        response_node)\n",
    "    \n",
    "    .add_edge(START,            \"retrieve\")\n",
    "    .add_edge(\"retrieve\",       \"full_retrieval\")\n",
    "    # .add_edge(\"retrieve\",       \"confirm\")\n",
    "    # .add_conditional_edges(\n",
    "    #     \"confirm\",\n",
    "    #     path=lambda out: out.get(\"goto\"),\n",
    "    #     path_map={\"full_retrieval\":\"full_retrieval\",\"retrieve\":\"retrieve\"},\n",
    "    # )\n",
    "    .add_edge(\"full_retrieval\", \"respond\")\n",
    "    .add_edge(\"respond\",         END)\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "graph.get_graph().print_ascii()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c5ec6-88c1-46c8-abb9-70eeeb385d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e108c1eb-d4e9-4f94-9a85-de8f098a6532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a496702-e3be-46f3-9194-a86d0653a043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "875ad30a-15c1-4828-8762-1b345db6a73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: What is the difference between Venture capital and Private Equity? \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'section_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_query}]}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# stream the response tokens as they arrive\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m msg_chunk, meta \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     14\u001b[0m     state,\n\u001b[1;32m     15\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m ):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanggraph_node\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# only print from your 'respond' node\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2527\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2526\u001b[0m             loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2527\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2528\u001b[0m             [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2529\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2530\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2531\u001b[0m             schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2532\u001b[0m         ):\n\u001b[1;32m   2533\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2534\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   2535\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m, in \u001b[0;36mfull_retrieval_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull_retrieval_node\u001b[39m(state: GraphState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# load full text only after confirmation\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     keys \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msection_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m     full_ctx \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sec \u001b[38;5;129;01min\u001b[39;00m keys:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'section_keys'",
      "\u001b[0mDuring task with name 'full_retrieval' and id '9efd7143-bfdf-b775-24b3-ac1e4a1846b6'"
     ]
    }
   ],
   "source": [
    "# assume `graph` is already defined and compiled as in your notebook\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"You: What is the difference between Venture capital and Private Equity?\")\n",
    "    if user_query.strip().lower() == \"exit\":\n",
    "        print(\"Exiting chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # build the LangGraph input\n",
    "    state = {\"messages\": [{\"role\": \"user\", \"content\": user_query}]}\n",
    "    \n",
    "    # stream the response tokens as they arrive\n",
    "    for msg_chunk, meta in graph.stream(\n",
    "        state,\n",
    "        stream_mode=\"messages\"\n",
    "    ):\n",
    "        print(meta.get(\"langgraph_node\"))\n",
    "        # only print from your 'respond' node\n",
    "        if meta.get(\"langgraph_node\") == \"respond\" and msg_chunk.content:\n",
    "            print(msg_chunk.content, end=\"\", flush=True)\n",
    "    print()  # newline after the full response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add293c2-758a-496a-a128-ea2bc66a114f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d34c8f-e3cc-48c7-8fb3-8ef70c6ac82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07152c31-466c-4ee5-a360-56e4e81f7172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc53ba7-852f-4bc9-8f7b-5662b13bedc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf151b-1601-42ab-9104-e86e0ae87e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5a986c-462e-4abb-8653-30e9cecae6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+  \n",
      "| __start__ |  \n",
      "+-----------+  \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "+----------+   \n",
      "| retrieve |   \n",
      "+----------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | respond |   \n",
      " +---------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | __end__ |   \n",
      " +---------+   \n"
     ]
    }
   ],
   "source": [
    "graph_builder = StateGraph(GraphState)\n",
    "graph_builder.add_node(\"retrieve\", retrieval_node)\n",
    "graph_builder.add_node(\"respond\", response_node)\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"respond\", END)\n",
    "\n",
    "graph_builder.add_edge(\"retrieve\", \"respond\")\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "graph.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00f75174-1af1-46b6-b45f-fa1600713a16",
   "metadata": {},
   "source": [
    "state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how can I start?\"}]\n",
    "}\n",
    "\n",
    "# Loop to handle continuous conversation\n",
    "while True:\n",
    "    # Execute the graph\n",
    "    state = graph.invoke(state)\n",
    "    \n",
    "    # Display the assistant's response\n",
    "    print(\"Assistant:\", state[\"response\"])\n",
    "    \n",
    "    # Get the next user input\n",
    "    user_input = input(\"User: \")\n",
    "    \n",
    "    # Append the user's message to the messages list\n",
    "    state[\"messages\"].append({\"role\": \"user\", \"content\": user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240a075-38a3-4b9d-8aeb-caff3ccf48b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85a0b2-6f17-4ea2-b7d4-4f986380f5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277901fe-f202-47ee-957c-eef0bdd838e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a856935-564e-4ce4-a294-d1562124b91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "38aebef8-830c-4772-b2cb-aee7664698bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Venture capital and Private Equity (PE) are two distinct investment strategies with significant differences in their approach, focus, and time horizons.\n",
      "\n",
      "**Venture Capital:**\n",
      "- **Definition:** Venture capital refers to funds raised by private companies for various purposes such as expansion, acquisition, debt repayment, or financial distress relief.\n",
      "- **Focus:** Typically involves the early stages of a company's life cycle, especially startups. It is often tied to specific industries and sectors.\n",
      "- **Investment Timing:** Focuses on short-term needs, such as scaling up a product or entering new markets.\n",
      "- **Management:** Often managed by individuals, family members, or government entities, not necessarily part of the company's management team.\n",
      "\n",
      "**Private Equity (PE):**\n",
      "- **Definition:** Private equity involves investing in companies with less than 20% control. These are usually established firms with significant assets under management.\n",
      "- **Focus:** Typically involves more stable investments over longer periods, especially when companies reach IPOs or go public.\n",
      "- **Investment Timing:** Provides returns over longer periods, particularly during the IPO phase or upon company sale to private equity funds.\n",
      "- **Management:** Usually managed by professionals with expertise in the industry, often from top-tier firms or hedge funds.\n",
      "\n",
      "In summary, venture capital is more focused on early-stage companies and startups, while PE involves broader investments across various sectors, offering returns over longer periods.\n",
      "CPU times: user 572 ms, sys: 528 ms, total: 1.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "query = \"What is the difference between Venture capital and Private Equity?\"\n",
    "\n",
    "initial_state = {\"query\": query}\n",
    "\n",
    "final_state = graph.invoke(initial_state)\n",
    "print(final_state[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b928dc-9a91-4b6a-82e5-fbc3fc048570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "47252189-078c-44d3-9abd-4d98709102ae",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "query = \"Compare Capital Markets Regulation in Europe vs USA?\"\n",
    "\n",
    "initial_state = {\"query\": query}\n",
    "\n",
    "final_state = graph.invoke(initial_state)\n",
    "print(final_state[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22be07d-501d-4b3b-bd78-b8c364873449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2469606-3878-4494-ae15-2e313180c17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1f996-9445-45da-bcca-6c2bc0836f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea00a35-2485-4546-acec-f605f865be64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
